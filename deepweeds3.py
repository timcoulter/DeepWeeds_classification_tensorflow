import argparse, os, requests, csv, psutil, os, sys, psutil, gc, tensorflow, keras
from zipfile import ZipFile
from pynvml.smi import nvidia_smi
import numpy as np
from keras import backend as K
import pandas as pd

import keras.models
from time import time
from datetime import datetime
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger
from keras.optimizers import Adam
from keras.models import Model, load_model
from keras.layers import Dense, GlobalAveragePooling2D
import tensorflow as tf

#Enter the full location of the root directory HERE
#ROOT_DIRECTORY = ""

# Global paths

OUTPUT_DIRECTORY = os.path.join(ROOT_DIRECTORY, "outputs")
LABEL_DIRECTORY = os.path.join(ROOT_DIRECTORY, "labels")
MODEL_DIRECTORY = os.path.join(ROOT_DIRECTORY, "models")
IMG_DIRECTORY = os.path.join(ROOT_DIRECTORY, "images")
DATA_DIRECTORY = os.path.join(ROOT_DIRECTORY, "data_split")

TRAIN_DIRECTORY = os.path.join(DATA_DIRECTORY, "train")
VAL_DIRECTORY = os.path.join(DATA_DIRECTORY, "val")
TEST_DIRECTORY = os.path.join(DATA_DIRECTORY, "test")

# Global variables
RAW_IMG_SIZE = (256, 256)
IMG_SIZE = (224, 224)
INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)
MAX_EPOCH = 500
BATCH_SIZE = 128
STOPPING_PATIENCE = 10
MAX_RESTARTS = 10  # Set the maximum number of restarts
LR_PATIENCE = 16
INITIAL_LR = 0.0001
CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8]
CLASSES = ['0', '1', '2', '3', '4', '5', '6', '7', '8']
CLASS_NAMES = ['Chinee Apple',
               'Lantana',
               'Parkinsonia',
               'Parthenium',
               'Prickly Acacia',
               'Rubber Vine',
               'Siam Weed',
               'Snake Weed',
               'Negatives']

nvsmi = nvidia_smi.getInstance()
GPU_MEM = nvsmi.DeviceQuery('memory.free, memory.total')
GPU_MEMORY = GPU_MEM['gpu'][0]['fb_memory_usage']['free'] *  0.00104858
print()


def ordered_filesize_list(directory_path, VERBOSE):
    # Get a list of filenames in the directory
    file_list = os.listdir(directory_path)
    output = []

    # Create a list of tuples containing filename and corresponding file size
    file_size_list = [(filename, os.path.getsize(os.path.join(directory_path, filename))) for filename in file_list]

    # Sort the list of tuples based on file size (second element of each tuple)
    sorted_file_size_list = sorted(file_size_list, key=lambda x: x[1])

    # Print the sorted list
    for filename, size in sorted_file_size_list:
        if VERBOSE != 2:
            print(f"{filename}: {size} bytes")
        output.append(filename)

    return output


def crop(img, size):
    """
    Crop the image concentrically to the desired size.
    :param img: Input image
    :param size: Required crop image size
    :return:
    """
    (h, w, c) = img.shape
    x = int((w - size[0]) / 2)
    y = int((h - size[1]) / 2)
    return img[y:(y + size[1]), x:(x + size[0]), :]


def crop_generator(batches, size):
    """
    Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator
    :param batches: Batches of images to be cropped
    :param size: Size to be cropped to
    :return:
    """
    while True:
        batch_x, batch_y = next(batches)
        (b, h, w, c) = batch_x.shape
        batch_crops = np.zeros((b, size[0], size[1], c))
        for i in range(b):
            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))
        yield (batch_crops, batch_y)


def compute_accuracy(y_true, y_pred):
    # Ensure both lists have the same length
    if len(y_true) != len(y_pred):
        raise ValueError("Input lists must have the same length.")

    correct_predictions = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)
    total_predictions = len(y_true)

    accuracy = correct_predictions / total_predictions
    return accuracy


def compute_precision(y_true, y_pred):
    true_positives = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)
    false_positives = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)

    if true_positives + false_positives == 0:
        return 0.0  # Avoid division by zero

    precision = true_positives / (true_positives + false_positives)
    return precision


def compute_recall(y_true, y_pred):
    true_positives = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)
    false_negatives = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)

    if true_positives + false_negatives == 0:
        return 0.0  # Avoid division by zero

    recall = true_positives / (true_positives + false_negatives)
    return recall


def compute_f1_score(y_true, y_pred):
    precision = compute_precision(y_true, y_pred)
    recall = compute_recall(y_true, y_pred)

    if precision + recall == 0:
        return 0.0  # Avoid division by zero

    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score


def load_model(model_name):
    try:
        mname = model_name.split('/')[-1].split('.')[0]
        # get outputs
        base_model = keras.models.load_model(model_name)
        print(mname)
        print("Encoder Params: {0}".format(base_model.count_params()))

        x = base_model.output
        # Add a global average pooling layer
        x = GlobalAveragePooling2D(name='avg_pool')(x)
        # Add fully connected output layer with sigmoid activation for multi label classification
        outputs = Dense(len(CLASSES), activation='sigmoid', name='fc9')(x)
        # Assemble the modified model
        model = Model(inputs=base_model.input, outputs=outputs)
        model._name = mname
        print("Total Params: {0}".format(model.count_params()))
        return model
    except Exception as E:
        print("Failed: {0}".format(mname))
        print(E)
        print()
        return None


def train(model_name):
    try:
        mname = os.path.basename(model_name).split('.')[0]
        # mname = model_name.split('/')[-1].split('.')[0]
        # get outputs
        output_folders = os.listdir(OUTPUT_DIRECTORY)
        base_model = keras.models.load_model(model_name)
        x = base_model.output
        # Add a global average pooling layer
        x = GlobalAveragePooling2D(name='avg_pool')(x)
        # Add fully connected output layer with sigmoid activation for multi label classification
        outputs = Dense(len(CLASSES), activation='sigmoid', name='fc9')(x)
        # Assemble the modified model
        model = Model(inputs=base_model.input, outputs=outputs)
        model._name = mname

    except Exception as E:
        print("Failed: {0}".format(mname))
        print(E)
        print()
        return

    print("Training: {0}".format(mname))

    ideal_batch_size = compute_ideal_batch_size(GPU_MEMORY, model, overhead_factor=1.25)
    if ideal_batch_size == 0:
        print("Model too large for GPU")
        return
    ideal_batch_size = next_lowest_power_of_2(ideal_batch_size)
    if ideal_batch_size > 256:
        ideal_batch_size = 256
    print("{0} : {1}".format(os.path.basename(mname), ideal_batch_size))
    BATCH_SIZE = ideal_batch_size
    print("memory: {0}, model: {1}, batch_size : {2}".format(GPU_MEMORY, mname, ideal_batch_size))

    # Create new output directory for individual folds from timestamp
    timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')
    output_directory = "{}/{}_B{}{}/".format(OUTPUT_DIRECTORY, timestamp, BATCH_SIZE,  '_' + mname)
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    train_files = []
    train_labels = []
    val_files = []
    val_labels = []

    for root, dirs, files in os.walk(TRAIN_DIRECTORY):
        for filename in files:
            train_files.append(os.path.join(root, filename))
            train_labels.append(os.path.split(root)[-1])

    for root, dirs, files in os.walk(VAL_DIRECTORY):
        for filename in files:
            val_files.append(os.path.join(root, filename))
            val_labels.append(os.path.split(root)[-1])

    train_image_count = len(train_files)
    val_image_count = len(val_files)

    # Training image augmentation
    train_data_generator = ImageDataGenerator(
        rescale=1. / 255,
        fill_mode="constant",
        shear_range=0.2,
        zoom_range=(0.5, 1),
        horizontal_flip=True,
        rotation_range=360,
        channel_shift_range=25,
        brightness_range=(0.75, 1.25))

    # Validation image augmentation
    val_data_generator = ImageDataGenerator(
        rescale=1. / 255,
        fill_mode="constant",
        shear_range=0.2,
        zoom_range=(0.5, 1),
        horizontal_flip=True,
        rotation_range=360,
        channel_shift_range=25,
        brightness_range=(0.75, 1.25))

    # Load train images in batches from directory and apply augmentations
    train_data_generator = train_data_generator.flow_from_directory(
        directory=TRAIN_DIRECTORY,
        target_size=RAW_IMG_SIZE,
        batch_size=BATCH_SIZE,
        shuffle=True,
        class_mode='categorical',
        seed=0)

    # Load validation images in batches from directory and apply rescaling
    val_data_generator = val_data_generator.flow_from_directory(
        directory=TEST_DIRECTORY,
        target_size=RAW_IMG_SIZE,
        batch_size=BATCH_SIZE,
        shuffle=True,
        class_mode='categorical',
        seed=0)

    # Crop augmented images from 256x256 to 224x224
    train_data_generator = crop_generator(train_data_generator, IMG_SIZE)
    val_data_generator = crop_generator(val_data_generator, IMG_SIZE)

    # Checkpoints for training
    model_checkpoint = ModelCheckpoint(output_directory + "lastbest-0.hdf5", verbose=1, save_best_only=True)
    early_stopping = EarlyStopping(patience=STOPPING_PATIENCE, restore_best_weights=True)
    tensorboard = TensorBoard(log_dir=output_directory, histogram_freq=0, write_graph=True, write_images=False)
    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=LR_PATIENCE, min_lr=0.000003125)
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=INITIAL_LR),
                  metrics=['categorical_accuracy'])

    # Train model until MAX_EPOCH, restarting after each early stop when learning has plateaued
    global_epoch = 0
    restarts = 0
    last_best_losses = []
    last_best_epochs = []

    csv_logger = CSVLogger(output_directory + "training_metrics.csv", append=True)

    while global_epoch < MAX_EPOCH and restarts < MAX_RESTARTS:
        history = model.fit(
            train_data_generator,
            steps_per_epoch=train_image_count // BATCH_SIZE,
            epochs=MAX_EPOCH - global_epoch,
            validation_data=val_data_generator,
            validation_steps=val_image_count // BATCH_SIZE,
            callbacks=[tensorboard, model_checkpoint, early_stopping, reduce_lr, csv_logger],
            shuffle=False,
            verbose=VERBOSE)
        last_best_losses.append(min(history.history['val_loss']))
        last_best_local_epoch = history.history['val_loss'].index(min(history.history['val_loss']))
        last_best_epochs.append(global_epoch + last_best_local_epoch)
        if early_stopping.stopped_epoch == 0:
            print("Completed training after {} epochs.".format(MAX_EPOCH))
            break
        else:
            global_epoch = global_epoch + early_stopping.stopped_epoch - STOPPING_PATIENCE + 1
            print("Early stopping triggered after local epoch {} (global epoch {}).".format(
                early_stopping.stopped_epoch, global_epoch))
            print("Restarting from last best val_loss at local epoch {} (global epoch {}).".format(
                early_stopping.stopped_epoch - STOPPING_PATIENCE, global_epoch - STOPPING_PATIENCE))
            restarts = restarts + 1
            model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR / 2 ** restarts),
                          metrics=['categorical_accuracy'])
            model_checkpoint = ModelCheckpoint(output_directory + "lastbest-{}.hdf5".format(restarts),
                                               monitor='val_loss', verbose=1, save_best_only=True, mode='min')

    # Save last best model info
    with open(output_directory + "last_best_models.csv", 'w', newline='') as file:
        writer = csv.writer(file, delimiter=',')
        writer.writerow(['Model file', 'Global epoch', 'Validation loss'])
        for i in range(restarts + 1):
            try:
                writer.writerow(["lastbest-{}.hdf5".format(i), last_best_epochs[i], last_best_losses[i]])
            except Exception:
                writer.writerow(["lastbest-{}.hdf5".format(i), "", ""])

    # Load the last best model
    model = load_model(
        output_directory + "lastbest-{}.hdf5".format(last_best_losses.index(min(last_best_losses))))

    test(output_directory)
    inference(output_directory)


def test(model_dir):
    # find_best_model
    models = [f for f in os.listdir(model_dir) if '.hdf5' in f]
    model_nums = [int(f.split('-')[1].split('.')[0]) for f in models]
    model_paths = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if '.hdf5' in f]
    best_model = model_paths[model_nums.index(max(model_nums))]

    model = keras.models.load_model(best_model)
    mname = model._name
    print("Testing: {0}".format(mname))

    output_directory = model_dir

    test_files = []
    test_labels = []

    for root, dirs, files in os.walk(TEST_DIRECTORY):
        for filename in files:
            test_files.append(os.path.join(root, filename))
            test_labels.append(os.path.split(root)[-1])
    test_image_count = len(test_files)

    test_data_generator = ImageDataGenerator(rescale=1. / 255)
    test_data_generator = test_data_generator.flow_from_directory(
        directory=TEST_DIRECTORY,
        target_size=IMG_SIZE,
        batch_size=1,
        shuffle=False,
        class_mode='categorical')

    # Evaluate model on test subset for kth fold
    predictions = model.predict(test_data_generator, steps=test_image_count, verbose=0)
    y_true = test_data_generator.classes
    y_pred = np.argmax(predictions, axis=1)
    y_pred[np.max(predictions, axis=1) < 1 / 9] = 8  # Assign predictions worse than random guess to negative class

    test_predictions_path = os.path.join(output_directory, "test_predictions.csv")
    if os.path.exists(test_predictions_path):
        os.remove(test_predictions_path)

    with open(test_predictions_path, 'w', newline='') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(["filename", "y_pred", "y_true"])
        for i in range(len(y_pred)):
            line = [os.path.basename(test_data_generator.filenames[i]), y_pred[i], test_data_generator.classes[i]]
            csv_writer.writerow(line)

    # make test summary
    test_summary_path = os.path.join(output_directory, "test_summary.csv")
    if os.path.exists(test_summary_path):
        os.remove(test_summary_path)
        print(f'{test_summary_path} deleted.')

    num_params = model.count_params()

    headings = ['Model', 'Parameters', 'Accuracy', 'F1-Score', 'Precision', 'Recall']

    metrics = [mname,
               num_params,
               compute_accuracy(y_true, y_pred),
               compute_f1_score(y_true, y_pred),
               compute_precision(y_true, y_pred),
               compute_recall(y_true, y_pred)]

    data_lines = [headings, metrics]

    with open(test_summary_path, 'w', newline='') as csvfile:
        csv_writer = csv.writer(csvfile)
        for line in data_lines:
            csv_writer.writerow(line)


def inference(model_dir):
    # find_best_model
    models = [f for f in os.listdir(model_dir) if '.hdf5' in f]
    model_nums = [int(f.split('-')[1].split('.')[0]) for f in models]
    model_paths = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if '.hdf5' in f]
    best_model = model_paths[model_nums.index(max(model_nums))]

    model = keras.models.load_model(best_model)
    mname = model._name
    print("Inference: {0}".format(mname))

    # Create new output directory for saving inference times
    timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')
    output_directory = "{}/{}_{}_inference/".format(OUTPUT_DIRECTORY, timestamp, mname)
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Load DeepWeeds dataframe

    filenames = [os.path.join(IMG_DIRECTORY, f) for f in os.listdir(IMG_DIRECTORY)]
    image_count = len(filenames)

    preprocessing_times = []
    inference_times = []
    for i in range(image_count):
        # Load image
        start_time = time()
        img = tf.io.read_file(filenames[i])
        img = tf.io.decode_image(img)
        # Resize to 224x224
        img = tf.image.resize(img, (224, 224))
        # Map to batch
        img = np.expand_dims(img, axis=0)
        # Scale from int to float
        img = img * 1. / 255
        preprocessing_time = time() - start_time
        start_time = time()
        # Predict label
        prediction = model.predict(img, batch_size=1, verbose=0)
        y_pred = np.argmax(prediction, axis=1)
        y_pred[np.max(prediction, axis=1) < 1 / 9] = 8
        inference_time = time() - start_time
        # Append times to lists
        preprocessing_times.append(preprocessing_time)
        inference_times.append(inference_time)
        if i == 1000:
            break

    # Save inference times to csv
    inference_path = output_directory + "tf_inference_times.csv"
    if os.path.exists(inference_path):
        os.remove(inference_path)

    with open(inference_path, 'w', newline='') as file:
        writer = csv.writer(file, delimiter=',')
        writer.writerow(['Filename', 'Preprocessing time (ms)', 'Inference time (ms)'])
        for i in range(len(inference_times)):
            writer.writerow([os.path.basename(filenames[i]), preprocessing_times[i] * 1000, inference_times[i] * 1000])

def keras_model_memory_usage_in_bytes(model, *, batch_size: int):
    """
    Return the estimated memory usage of a given Keras model in bytes.
    This includes the model weights and layers, but excludes the dataset.

    The model shapes are multipled by the batch size, but the weights are not.

    Args:
        model: A Keras model.
        batch_size: The batch size you intend to run the model with. If you
            have already specified the batch size in the model itself, then
            pass `1` as the argument here.
    Returns:
        An estimate of the Keras model's memory usage in bytes.

    """
    default_dtype = tf.keras.backend.floatx()
    shapes_mem_count = 0
    internal_model_mem_count = 0
    for layer in model.layers:
        if isinstance(layer, tf.keras.Model):
            internal_model_mem_count += keras_model_memory_usage_in_bytes(
                layer, batch_size=batch_size
            )
        single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size
        out_shape = layer.output_shape
        if isinstance(out_shape, list):
            out_shape = out_shape[0]
        for s in out_shape:
            if s is None:
                continue
            single_layer_mem *= s
        shapes_mem_count += single_layer_mem

    trainable_count = sum(
        [tf.keras.backend.count_params(p) for p in model.trainable_weights]
    )
    non_trainable_count = sum(
        [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]
    )

    total_memory = (
        batch_size * shapes_mem_count
        + internal_model_mem_count
        + trainable_count
        + non_trainable_count
    )
    return total_memory

def compute_ideal_batch_size(gpu_memory, model, overhead_factor=1.25):
    """
    Computes the ideal batch size based on available GPU memory and model memory requirements.

    Args:
        gpu_memory (float): Available GPU memory in gigabytes.
        model (tf.keras.Model): TensorFlow model.
        overhead_factor (float, optional): A factor to account for additional memory usage during training. Default is 1.5.

    Returns:
        int: Ideal batch size.
    """
    model_mem = keras_model_memory_usage_in_bytes(model=model, batch_size=1)*1e-9
    if (gpu_memory - model_mem*overhead_factor) <= 0:
        print("Model is too large for GPU")
        return 0

    i=0
    while (gpu_memory - model_mem*overhead_factor) > 0:
        i=i+1
        model_mem = keras_model_memory_usage_in_bytes(model=model, batch_size=i) * 1e-9

    return i

def next_lowest_power_of_2(number):
    if number <= 0:
        return 0

    power = 1
    while power < number:
        power <<= 1  # Equivalent to power = power * 2

    return power >> 1  # Equivalent to power // 2

def compute_average_inference_time(inference_dir):
    df = pd.read_csv(os.path.join(inference_dir, 'tf_inference_times.csv'), index_col=None)
    # df = df.drop(index=0)
    # df = df.reset_index(drop=True)

    # Calculate the mean and standard deviation for each column
    column_means = df.mean()
    column_stds = df.std()

    # Define the number of standard deviations for identifying outliers
    num_std_devs = 2  # Adjust as needed

    # Identify and filter out rows with values outside the defined range
    filtered_df = df[~((df - column_means).abs() > num_std_devs * column_stds)]
    filtered_df = filtered_df.dropna()
    mean_df = filtered_df.mean().tolist()
    return mean_df

def create_overall_test_summary(directory):
    matching_paths = []

    for dirpath, _, filenames in os.walk(directory):
        for fname in filenames:
            if fname == 'test_summary.csv':
                matching_paths.append(os.path.join(dirpath, fname))

    dfs = [pd.read_csv(f, index_col=None) for f in matching_paths]
    test_summary_df = pd.concat(dfs, ignore_index=True)
    test_summary_df['preprocessing_time_ms'] = np.nan
    test_summary_df['inference_time_ms'] = np.nan

    inference_times = []
    inference_dirs = [os.path.join(directory, f) for f in os.listdir(directory) if '_inference' in f]
    for i in range(len(inference_dirs)):
        times = compute_average_inference_time(inference_dirs[i])

        # find which row to add the times to
        model_name = inference_dirs[i].split('/')[-1].replace('_inference', '').split('_')[-1]
        try:
            index_model = test_summary_df.loc[test_summary_df['Model'] == model_name].index[0]
            test_summary_df.loc[index_model, 'preprocessing_time_ms'] = times[0]
            test_summary_df.loc[index_model, 'inference_time_ms'] = times[1]
        except Exception:
            pass

    test_summary_path = os.path.join(directory, "overall_test_summary.csv")
    if os.path.exists(test_summary_path):
        os.remove(test_summary_path)
    test_summary_df.to_csv(test_summary_path, index=False)



if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="DeepWeeds Script")
    parser.add_argument("--idx", type=int, default=0, help="Index of the model to train (0-based)")
    parser.add_argument("--step", type=int, default=1, help="Model index step size")
    parser.add_argument("--compute_batch_size", type=int, default=0, help="Compute batch sizes for available models")
    parser.add_argument("--compute_overall_summary", type=int, default=1, help="Compute Test Summary")
    args = parser.parse_args()

    print(MODEL_DIRECTORY)

    ordered_models = ordered_filesize_list(MODEL_DIRECTORY, VERBOSE)
    all_models = [os.path.join(MODEL_DIRECTORY, f) for f in ordered_models]

    gpu_available = tf.config.experimental.list_physical_devices('GPU')

    if args.compute_overall_summary == 1:
        create_overall_test_summary(OUTPUT_DIRECTORY)

    if gpu_available:
        print("GPU is available")
        # Get GPU information
        gpu_info = tf.config.experimental.get_visible_devices('GPU')
        for device in gpu_info:
            print("GPU Name:", device.name)
            print("GPU Type:", device.device_type)

        if args.compute_batch_size == 1:

            for i in range(len(all_models)):
                model = load_model(all_models[i])
                if model == None:
                    continue
                ideal_batch_size = compute_ideal_batch_size(GPU_MEMORY, model, overhead_factor=1.25)
                ideal_batch_size = next_lowest_power_of_2(ideal_batch_size)
                print("memory: {0}, model: {1}, batch_size : {2}".format(GPU_MEMORY,os.path.basename(all_models[i]), ideal_batch_size))
                print()
    else:
        print("No GPU is available")

    trained_models = [f.replace('_inference','').split('_')[-1] for f in os.listdir(OUTPUT_DIRECTORY) if '_inference' in f]

    if args.compute_batch_size==0:
        for i in range(args.idx, len(all_models), args.step):
            mname = os.path.basename(all_models[i]).split('.')[0]
            if mname not in trained_models:
                train(all_models[i])



